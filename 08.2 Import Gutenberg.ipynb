{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, defaultdict\n",
    "from gutenberg.acquire.text import UnknownDownloadUriException\n",
    "import re\n",
    "from gensim.utils import tokenize\n",
    "import random\n",
    "import nltk\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twain, Mark', 1835, 210),\n",
       " ('Ebers, Georg', 1837, 164),\n",
       " ('Parker, Gilbert', 1862, 135),\n",
       " ('Fenn, George Manville', 1831, 128),\n",
       " ('Jacobs, W. W. (William Wymark)', 1863, 112)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/gutenberg_index.json') as fin:\n",
    "    authors = json.load(fin)\n",
    "recent = [x for x in authors if 'birthdate' in x and x['birthdate'] > 1830]\n",
    "[(x['name'], x['birthdate'], x['english_books']) for x in recent[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('author', 'formaturi', 'language', 'rights', 'subject', 'title')\n"
     ]
    }
   ],
   "source": [
    "print(list_supported_metadatas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAPH_SPLIT_RE = re.compile(r'\\n *\\n+')\n",
    "\n",
    "def extract_conversations(text, quote='\"'):\n",
    "    paragraphs = PARAGRAPH_SPLIT_RE.split(text.strip())\n",
    "    conversations = [['']]\n",
    "    for paragraph in paragraphs:\n",
    "        chunks = paragraph.replace('\\n', ' ').split(quote)\n",
    "        for i in range((len(chunks) + 1) // 2):\n",
    "            if (len(chunks[i * 2]) > 100 or len(chunks) == 1) and conversations[-1] != ['']:\n",
    "                if conversations[-1][-1] == '':\n",
    "                    del conversations[-1][-1]\n",
    "                conversations.append([''])\n",
    "            if i * 2 + 1 < len(chunks):\n",
    "                chunk = chunks[i * 2 + 1]\n",
    "                if chunk:\n",
    "                    if conversations[-1][-1]:\n",
    "                        if chunk[0] >= 'A' and chunk[0] <= 'Z':\n",
    "                            if conversations[-1][-1].endswith(','):\n",
    "                                conversations[-1][-1] = conversations[-1][-1][:-1]\n",
    "                            conversations[-1][-1] += '.'\n",
    "                        conversations[-1][-1] += ' '\n",
    "                    conversations[-1][-1] += chunk\n",
    "        if conversations[-1][-1]:\n",
    "            conversations[-1].append('')\n",
    "\n",
    "    return [x for x in conversations if len(x) > 1]\n",
    "\n",
    "\n",
    "conversations = extract_conversations(strip_headers(load_etext(10008).strip()))\n",
    "sum(len(x) for x in conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646779 15349\n"
     ]
    }
   ],
   "source": [
    "LATIN_1_CHARS = (\n",
    "    (u'\\xe2\\x80\\x99', \"'\"),\n",
    "    (u'\\xc3\\xa9', 'e'),\n",
    "    (u'\\xe2\\x80\\x90', '-'),\n",
    "    (u'\\xe2\\x80\\x91', '-'),\n",
    "    (u'\\xe2\\x80\\x92', '-'),\n",
    "    (u'\\xe2\\x80\\x93', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x98', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9b', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9d', '\"'),\n",
    "    (u'\\xe2\\x80\\x9e', '\"'),\n",
    "    (u'\\xe2\\x80\\x9f', '\"'),\n",
    "    (u'\\xe2\\x80\\xa6', '...'),\n",
    "    (u'\\xe2\\x80\\xb2', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb3', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb4', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb5', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb6', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb7', \"'\"),\n",
    "    (u'\\xe2\\x81\\xba', \"+\"),\n",
    "    (u'\\xe2\\x81\\xbb', \"-\"),\n",
    "    (u'\\xe2\\x81\\xbc', \"=\"),\n",
    "    (u'\\xe2\\x81\\xbd', \"(\"),\n",
    "    (u'\\xe2\\x81\\xbe', \")\")\n",
    ")\n",
    "\n",
    "books = 0\n",
    "for author in recent[:1000]:\n",
    "    for book in author['books']:\n",
    "        books += 1\n",
    "        try:\n",
    "            txt = strip_headers(load_etext(int(book[0]))).strip()\n",
    "        except UnknownDownloadUriException:\n",
    "            continue\n",
    "        for ch1, ch2 in LATIN_1_CHARS:\n",
    "            txt = txt.replace(ch1, ch2)\n",
    "        conversations += extract_conversations(txt)\n",
    "\n",
    "print(len(conversations), books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gutenberg.txt', 'w') as fout:\n",
    "    for conv in conversations:\n",
    "        fout.write('\\n'.join(conv) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile('(\\w+|\\?)', re.UNICODE)\n",
    "token_counter = Counter()\n",
    "with open('gutenberg.txt') as fin:\n",
    "    for line in fin:\n",
    "        line = line.lower().replace('_', ' ')\n",
    "        token_counter.update(RE_TOKEN.findall(line))\n",
    "with open('gutenberg.tok', 'w') as fout:\n",
    "    for token, count in token_counter.items():\n",
    "        fout.write('%s\\t%d\\n' % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2674921"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'd'),\n",
       " ('I', 'I'),\n",
       " ('I', 'I'),\n",
       " ('ve', 'e'),\n",
       " ('got', 't'),\n",
       " ('to', 'o'),\n",
       " ('arrest', 't'),\n",
       " ('him', 'm'),\n",
       " ('in', 'n'),\n",
       " ('my', 'y'),\n",
       " ('own', 'n'),\n",
       " ('house', 'e'),\n",
       " ('I', 'I'),\n",
       " ('doubt', 't'),\n",
       " ('if', 'f'),\n",
       " ('you', 'u'),\n",
       " ('will', 'l'),\n",
       " ('have', 'e'),\n",
       " ('the', 'e'),\n",
       " ('opportunity', 'y'),\n",
       " ('sir', 'r')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT_ALPHABETIC.findall(conv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
