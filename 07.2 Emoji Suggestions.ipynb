{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets\n",
    "\n",
    "We can gather a sample of Twitter data using the Twitter API (https://dev.twitter.com).  To do so, we'll need to create a Twitter application and get credentials for it.  You can do this manually at https://app.twitter.com.  Once you have an app, go to the \"Key and Access Tokens\" tab to find your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import twitter\n",
    "import emoji\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "import tensorflow.keras.callbacks\n",
    "import json\n",
    "\n",
    "import os\n",
    "import nb_utils\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Average\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In 2018, the ladies of #thecollective landed 8 tv feature stories for our incredible clients TOTALING $347,793 in e… https://t.co/FxpnHeIQuQ',\n",
       " '@sakuraimaki3 merry Christmas✨🎄✨',\n",
       " 'دعاء السجود:سبوح قدوس رب الملائكة والروح https://t.co/HpibZCgbwi',\n",
       " 'こころ 配信中!!\\nhttps://t.co/Izcwf8aNJV',\n",
       " 'Vou comer pra caralho hoje Pprt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auth=twitter.OAuth(\n",
    "    consumer_key=CONSUMER_KEY,\n",
    "    consumer_secret=CONSUMER_SECRET,\n",
    "    token=ACCESS_TOKEN,\n",
    "    token_secret=ACCESS_SECRET,\n",
    ")\n",
    "\n",
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "list(itertools.islice(\n",
    "    (x['text'] for x in status_stream.sample() if x.get('text') is not None),\n",
    "    0,\n",
    "    5\n",
    "))\n",
    "# [x['text'] for x in itertools.islice(status_stream.sample(), 0, 5) if x.get('text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 676 ms, sys: 68.5 ms, total: 745 ms\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "def english_has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time tweets = list(itertools.islice(filter(english_has_emoji, status_stream.sample()), 0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped = []\n",
    "for tweet in tweets:\n",
    "    text = tweet['text']\n",
    "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
    "    if len(emojis) == 1:\n",
    "        emoiji = emojis.pop()\n",
    "        text = ''.join(ch for ch in text if ch != emoiji)\n",
    "        stripped.append((text, emoiji))\n",
    "len(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CNN\n",
    "\n",
    "Let's see what the CNN of the previous chapter does on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂    20061\n",
       "😭     8954\n",
       "❤     6802\n",
       "😍     4815\n",
       "🔥     3470\n",
       "🤣     3427\n",
       "🤔     2391\n",
       "🙏     2225\n",
       "😊     2005\n",
       "✨     1971\n",
       "💕     1969\n",
       "👀     1860\n",
       "🎉     1785\n",
       "😩     1769\n",
       "💜     1718\n",
       "🙄     1650\n",
       "💀     1630\n",
       "👏     1470\n",
       "😘     1346\n",
       "🖤     1334\n",
       "🙌     1324\n",
       "🎄     1302\n",
       "👍     1242\n",
       "💯     1204\n",
       "😔     1154\n",
       "😉     1118\n",
       "💖     1068\n",
       "😎     1063\n",
       "💙     1059\n",
       "😁      996\n",
       "     ...  \n",
       "🐃        1\n",
       "🥙        1\n",
       "🕗        1\n",
       "📴        1\n",
       "🌖        1\n",
       "🚺        1\n",
       "🏋        1\n",
       "🕐        1\n",
       "🚋        1\n",
       "⚱        1\n",
       "🌜        1\n",
       "🏗        1\n",
       "☸        1\n",
       "🥉        1\n",
       "↕        1\n",
       "🈵        1\n",
       "🎳        1\n",
       "💺        1\n",
       "🚝        1\n",
       "⛩        1\n",
       "↙        1\n",
       "📑        1\n",
       "⛑        1\n",
       "🚥        1\n",
       "🎽        1\n",
       "➰        1\n",
       "🐫        1\n",
       "⛹        1\n",
       "🔢        1\n",
       "📗        1\n",
       "Name: emoji, Length: 982, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets = pd.read_csv('data/emojis.txt')\n",
    "all_tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂    20061\n",
       "😭     8954\n",
       "❤     6802\n",
       "😍     4815\n",
       "🔥     3470\n",
       "🤣     3427\n",
       "🤔     2391\n",
       "🙏     2225\n",
       "😊     2005\n",
       "✨     1971\n",
       "💕     1969\n",
       "👀     1860\n",
       "🎉     1785\n",
       "😩     1769\n",
       "💜     1718\n",
       "🙄     1650\n",
       "💀     1630\n",
       "👏     1470\n",
       "😘     1346\n",
       "🖤     1334\n",
       "🙌     1324\n",
       "🎄     1302\n",
       "👍     1242\n",
       "💯     1204\n",
       "😔     1154\n",
       "😉     1118\n",
       "💖     1068\n",
       "😎     1063\n",
       "💙     1059\n",
       "Name: emoji, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
    "tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @missypbb_: Missy casually serving good loo...</td>\n",
       "      <td>😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah I just finished the heirs ️</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @JustinPTurner: I hurt my foot and my dog s...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you ain't shit</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @HoustonMay23: Christmas just got better ht...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how cute  https://t.co/C9rFaFU2b7</td>\n",
       "      <td>👀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @SuchAqueent: “Send me a pic” is where a wo...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Anna - Trio 1⃣2⃣3⃣  https://t.co/OWEZwo34BL\\nA...</td>\n",
       "      <td>😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @6blogger: I'll never get bored of sharing ...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @youbs19: Fuck winter wonderland come we go...</td>\n",
       "      <td>🤔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Anna - Trio 1⃣2⃣3⃣  https://t.co/OWEZwo34BL\\nA...</td>\n",
       "      <td>😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @chamwink: wanna one winter store doll phot...</td>\n",
       "      <td>😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@brennasparksxxx  It is the number you have in...</td>\n",
       "      <td>🤣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RT @raylewis: Blessed to have worn 52. 🏿  \\n\\n...</td>\n",
       "      <td>🙏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RT @itsaustonpugh: when i put \"\" this is what ...</td>\n",
       "      <td>👀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RT @KathNielMsia: Kath's IG Story https://t.co...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RT @PopCraveNet: .@IAmCardiB shares a sneak pe...</td>\n",
       "      <td>🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT @_NellaRose: I’m gonna make 0:43 my new cha...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RT @powerfeen: I wish wealth upon me and all m...</td>\n",
       "      <td>😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @Joey_Toccoo: So proud of this kid  my insp...</td>\n",
       "      <td>😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RT @DukeOfPhakalane: No one experiences self b...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RT @dailywbb: Spread the cuteness https://t.co...</td>\n",
       "      <td>💕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RT @ismadalilah: Thank you for this lovely ver...</td>\n",
       "      <td>😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lol I wish Ellen was one of my cool funny gay ...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RT @montenegro_emil: #ALDUB179thWeeksary \\n@ly...</td>\n",
       "      <td>😘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RT @JeffreeStar: The best medicine is being YOU.</td>\n",
       "      <td>✨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RT @jaybeeTRENDz: Yea. SUCKING THAT DICK  http...</td>\n",
       "      <td>🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RT @CalgaryPolice: We have an urgent #WarrantW...</td>\n",
       "      <td>🎄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>@Intruder_Gaming @BethesdaStudios I’ll trade u...</td>\n",
       "      <td>😉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RT @DamienRuiz5: Tomorrow night in SF we bring...</td>\n",
       "      <td>🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150841</th>\n",
       "      <td>RT @soompi: Happy Birthday to #MAMAMOO's Moonb...</td>\n",
       "      <td>🎉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150842</th>\n",
       "      <td>RT @SWildIifevids: World's smallest cat - Rust...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150848</th>\n",
       "      <td>RT @December_w3: “Muah, muah kisses to my bitc...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150849</th>\n",
       "      <td>RT @GraysonDolan: Wait what 7 MILLION THANK YO...</td>\n",
       "      <td>😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150851</th>\n",
       "      <td>@NaezrahMax This shit is fye  to me keep up th...</td>\n",
       "      <td>🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150852</th>\n",
       "      <td>RT @liedtocth: GIVEAWAY TIME\\n\\ni’m giving awa...</td>\n",
       "      <td>✨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150854</th>\n",
       "      <td>RT @rhytunnell: Serious question\\n\\nDo you put...</td>\n",
       "      <td>🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150855</th>\n",
       "      <td>This is too cute</td>\n",
       "      <td>😍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150856</th>\n",
       "      <td>@kimx090 i hope everything feels better soon :...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150858</th>\n",
       "      <td>#Hentai Buddies\\n\\n @DoujinsApp\\n @HentaiAdvis...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150859</th>\n",
       "      <td>@scoo9__  nigga ain't tryna tip the scale from...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150860</th>\n",
       "      <td>RT @FatimahSwiney: @JeNayTheDreamer Feel you</td>\n",
       "      <td>💯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150862</th>\n",
       "      <td>@wegottathing Listen to wegottathing podcast  ...</td>\n",
       "      <td>💕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150863</th>\n",
       "      <td>@phomrakperth Plan: “Perth come fast bro I can...</td>\n",
       "      <td>🤣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150864</th>\n",
       "      <td>Yesterday I got the “you’re the best mom”.  Al...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150865</th>\n",
       "      <td>RT @afiqqafnd: Guys help me , tolong RT je pls...</td>\n",
       "      <td>🙏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150866</th>\n",
       "      <td>RT @summerbromance: IT`S RAINING MEN, HALLELUJ...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150867</th>\n",
       "      <td>RT @wbbvibes: The Bear Rangers! ️🥰 https://t.c...</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150869</th>\n",
       "      <td>RT @Yomnasamiirr: I’m starting 2019 with nobody</td>\n",
       "      <td>🖤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150870</th>\n",
       "      <td>Won't @jack verify you?</td>\n",
       "      <td>🤔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150872</th>\n",
       "      <td>I love you</td>\n",
       "      <td>💖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150875</th>\n",
       "      <td>@Kels18Area Take the dead one</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150876</th>\n",
       "      <td>@VakoStrange  ok that’s the one i’m thinking o...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150880</th>\n",
       "      <td>RT @Kuvshinov_Ilya: One more cover I’ve done f...</td>\n",
       "      <td>✨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150881</th>\n",
       "      <td>RT @Drebae_: This a lil too accurate. It hurts...</td>\n",
       "      <td>💀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150882</th>\n",
       "      <td>RT @TheNewRight4:      It's all so tiresome ht...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150883</th>\n",
       "      <td>RT @MONEYMARC0: just put together my girl vani...</td>\n",
       "      <td>😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150884</th>\n",
       "      <td>RT @GeeDee215: Ugh.\\n\\n1.) this poor kid. \\n\\n...</td>\n",
       "      <td>😔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150885</th>\n",
       "      <td>I use to be scared of these people when they c...</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150886</th>\n",
       "      <td>@saetoven Ain no shame in my game</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83186 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text emoji\n",
       "0       RT @missypbb_: Missy casually serving good loo...     😍\n",
       "1                        Yeah I just finished the heirs ️     ❤\n",
       "2       RT @JustinPTurner: I hurt my foot and my dog s...     😂\n",
       "3                                         you ain't shit      😂\n",
       "4       RT @HoustonMay23: Christmas just got better ht...     😂\n",
       "5                       how cute  https://t.co/C9rFaFU2b7     👀\n",
       "10      RT @SuchAqueent: “Send me a pic” is where a wo...     😂\n",
       "11      Anna - Trio 1⃣2⃣3⃣  https://t.co/OWEZwo34BL\\nA...     😍\n",
       "12      RT @6blogger: I'll never get bored of sharing ...     😂\n",
       "13      RT @youbs19: Fuck winter wonderland come we go...     🤔\n",
       "15      Anna - Trio 1⃣2⃣3⃣  https://t.co/OWEZwo34BL\\nA...     😍\n",
       "19      RT @chamwink: wanna one winter store doll phot...     😍\n",
       "20      @brennasparksxxx  It is the number you have in...     🤣\n",
       "21      RT @raylewis: Blessed to have worn 52. 🏿  \\n\\n...     🙏\n",
       "22      RT @itsaustonpugh: when i put \"\" this is what ...     👀\n",
       "23      RT @KathNielMsia: Kath's IG Story https://t.co...     ❤\n",
       "24      RT @PopCraveNet: .@IAmCardiB shares a sneak pe...     🔥\n",
       "26      RT @_NellaRose: I’m gonna make 0:43 my new cha...     😂\n",
       "27      RT @powerfeen: I wish wealth upon me and all m...     😭\n",
       "29      RT @Joey_Toccoo: So proud of this kid  my insp...     😭\n",
       "30      RT @DukeOfPhakalane: No one experiences self b...     😂\n",
       "31      RT @dailywbb: Spread the cuteness https://t.co...     💕\n",
       "32      RT @ismadalilah: Thank you for this lovely ver...     😭\n",
       "33      Lol I wish Ellen was one of my cool funny gay ...     😂\n",
       "35      RT @montenegro_emil: #ALDUB179thWeeksary \\n@ly...     😘\n",
       "37      RT @JeffreeStar: The best medicine is being YOU.      ✨\n",
       "39      RT @jaybeeTRENDz: Yea. SUCKING THAT DICK  http...     🔥\n",
       "42      RT @CalgaryPolice: We have an urgent #WarrantW...     🎄\n",
       "44      @Intruder_Gaming @BethesdaStudios I’ll trade u...     😉\n",
       "47      RT @DamienRuiz5: Tomorrow night in SF we bring...     🔥\n",
       "...                                                   ...   ...\n",
       "150841  RT @soompi: Happy Birthday to #MAMAMOO's Moonb...     🎉\n",
       "150842  RT @SWildIifevids: World's smallest cat - Rust...     ❤\n",
       "150848  RT @December_w3: “Muah, muah kisses to my bitc...     😂\n",
       "150849  RT @GraysonDolan: Wait what 7 MILLION THANK YO...     😎\n",
       "150851  @NaezrahMax This shit is fye  to me keep up th...     🔥\n",
       "150852  RT @liedtocth: GIVEAWAY TIME\\n\\ni’m giving awa...     ✨\n",
       "150854  RT @rhytunnell: Serious question\\n\\nDo you put...     🙄\n",
       "150855                                  This is too cute      😍\n",
       "150856  @kimx090 i hope everything feels better soon :...     ❤\n",
       "150858  #Hentai Buddies\\n\\n @DoujinsApp\\n @HentaiAdvis...     ❤\n",
       "150859  @scoo9__  nigga ain't tryna tip the scale from...     😂\n",
       "150860      RT @FatimahSwiney: @JeNayTheDreamer Feel you      💯\n",
       "150862  @wegottathing Listen to wegottathing podcast  ...     💕\n",
       "150863  @phomrakperth Plan: “Perth come fast bro I can...     🤣\n",
       "150864  Yesterday I got the “you’re the best mom”.  Al...     ❤\n",
       "150865  RT @afiqqafnd: Guys help me , tolong RT je pls...     🙏\n",
       "150866  RT @summerbromance: IT`S RAINING MEN, HALLELUJ...     😂\n",
       "150867  RT @wbbvibes: The Bear Rangers! ️🥰 https://t.c...     ❤\n",
       "150869    RT @Yomnasamiirr: I’m starting 2019 with nobody     🖤\n",
       "150870                           Won't @jack verify you?      🤔\n",
       "150872                                         I love you     💖\n",
       "150875                     @Kels18Area Take the dead one      😂\n",
       "150876  @VakoStrange  ok that’s the one i’m thinking o...     😂\n",
       "150880  RT @Kuvshinov_Ilya: One more cover I’ve done f...     ✨\n",
       "150881  RT @Drebae_: This a lil too accurate. It hurts...     💀\n",
       "150882  RT @TheNewRight4:      It's all so tiresome ht...     😂\n",
       "150883  RT @MONEYMARC0: just put together my girl vani...     😎\n",
       "150884  RT @GeeDee215: Ugh.\\n\\n1.) this poor kid. \\n\\n...     😔\n",
       "150885  I use to be scared of these people when they c...     😂\n",
       "150886                 @saetoven Ain no shame in my game      😂\n",
       "\n",
       "[83186 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@KristenScholer NYSE-800 &amp;NASDAQ-200MARKETCRASH%\\n(FED RATES DECISION&amp;TRADE WITH CHINA AMERICAN&amp;WORLD OPPOSES&amp;BITCO… https://t.co/37efEyLeSr'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tweets['text'], key=lambda t:len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(chain(*tweets['text']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in tweets['text'])\n",
    "\n",
    "emojis = list(sorted(set(tweets['emoji'])))\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "emojis[:10]\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]),\n",
       " array([15.,  5., 15.,  0., 15.,  1., 15., 26.,  5., 15.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(tweets, batch_size):\n",
    "    while True:\n",
    "        if batch_size is None:\n",
    "            batch = tweets\n",
    "            batch_size = batch.shape[0]\n",
    "        else:\n",
    "            batch = tweets.sample(batch_size)\n",
    "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
    "            y[row_idx] = emoji_to_idx[row['emoji']]\n",
    "            for ch_idx, ch in enumerate(row['text']):\n",
    "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "        yield X, y\n",
    "\n",
    "next(data_generator(tweets, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:651: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_cnn_input (InputLayer)  [(None, 155, 1457)]       0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 150, 128)          1119104   \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "char_cnn_predictions (Dense) (None, 29)                3741      \n",
      "=================================================================\n",
      "Total params: 1,581,981\n",
      "Trainable params: 1,581,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 179s - loss: 2.7882 - acc: 0.2700\n",
      "Epoch 2/20\n",
      " - 170s - loss: 2.3591 - acc: 0.3804\n",
      "Epoch 3/20\n",
      " - 164s - loss: 2.1329 - acc: 0.4356\n",
      "Epoch 4/20\n",
      " - 165s - loss: 1.9182 - acc: 0.4891\n",
      "Epoch 5/20\n",
      " - 165s - loss: 1.7825 - acc: 0.5231\n",
      "Epoch 6/20\n",
      " - 169s - loss: 1.6476 - acc: 0.5584\n",
      "Epoch 7/20\n",
      " - 164s - loss: 1.5344 - acc: 0.5873\n",
      "Epoch 8/20\n",
      " - 164s - loss: 1.4285 - acc: 0.6120\n",
      "Epoch 9/20\n",
      " - 166s - loss: 1.3437 - acc: 0.6336\n",
      "Epoch 10/20\n",
      " - 169s - loss: 1.2503 - acc: 0.6594\n",
      "Epoch 11/20\n",
      " - 169s - loss: 1.1598 - acc: 0.6839\n",
      "Epoch 12/20\n",
      " - 166s - loss: 1.0705 - acc: 0.7065\n",
      "Epoch 13/20\n",
      " - 165s - loss: 0.9823 - acc: 0.7314\n",
      "Epoch 14/20\n",
      " - 168s - loss: 0.8942 - acc: 0.7544\n",
      "Epoch 15/20\n",
      " - 161s - loss: 0.8119 - acc: 0.7774\n",
      "Epoch 16/20\n",
      " - 169s - loss: 0.7379 - acc: 0.7981\n",
      "Epoch 17/20\n",
      " - 170s - loss: 0.6767 - acc: 0.8144\n",
      "Epoch 18/20\n",
      " - 166s - loss: 0.6115 - acc: 0.8328\n",
      "Epoch 19/20\n",
      " - 164s - loss: 0.5498 - acc: 0.8487\n",
      "Epoch 20/20\n",
      " - 165s - loss: 0.4878 - acc: 0.8655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8318613908>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "char_cnn_model.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) // BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6949294060468674, 0.5369873]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('zoo/07', exist_ok=True)\n",
    "\n",
    "with open('zoo/07/emoji_chars.json', 'w') as fout:\n",
    "    json.dump({\n",
    "        'emojis': ''.join(emojis),\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'max_sequence_len': max_sequence_len,\n",
    "    }, fout)\n",
    "char_cnn_model.save('zoo/07/char_cnn_model.h5')\n",
    "char_cnn_model.save_weights('zoo/07/char_cnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44191</th>\n",
       "      <td>i’m really gonna try to be celibate for all of 2019.</td>\n",
       "      <td>😭</td>\n",
       "      <td>🙏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58270</th>\n",
       "      <td>@_NEObts CUTIE</td>\n",
       "      <td>💕</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67486</th>\n",
       "      <td>RT @Essence: 2018 was definitely the year of Black women in politics! 🏾 https://t.co/q3v8Ccxixt</td>\n",
       "      <td>🙌</td>\n",
       "      <td>🙏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17855</th>\n",
       "      <td>RT @powerfeen: I wish wealth upon me and all my friends so this can be our only problem  https://t.co/WUi2Fx8aKR</td>\n",
       "      <td>😭</td>\n",
       "      <td>😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48687</th>\n",
       "      <td>@realdawnsummers @AnthonyCurtis68 i thought that was still legal in Mississippi \\n\\nkidding!!!!! lol... that's Alabama</td>\n",
       "      <td>🤔</td>\n",
       "      <td>👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60415</th>\n",
       "      <td>RT @itsaustonpugh: when i put \"\" this is what i really mean https://t.co/zPuraXzTLE</td>\n",
       "      <td>👀</td>\n",
       "      <td>👀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97450</th>\n",
       "      <td>RT @fine_ass_mari: I hate texting.. come live with me</td>\n",
       "      <td>😂</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13183</th>\n",
       "      <td>@HesterDressage @Hezzzza Wow what a lucky girl ️</td>\n",
       "      <td>❤</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131561</th>\n",
       "      <td>I want to starve myself</td>\n",
       "      <td>😂</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62357</th>\n",
       "      <td>RT @mytweetsaremid: Tupac really killed it here  https://t.co/Edt8dnDR7r</td>\n",
       "      <td>🔥</td>\n",
       "      <td>😭</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          text  \\\n",
       "44191                                                                    i’m really gonna try to be celibate for all of 2019.    \n",
       "58270                                                                                                          @_NEObts CUTIE    \n",
       "67486                          RT @Essence: 2018 was definitely the year of Black women in politics! 🏾 https://t.co/q3v8Ccxixt   \n",
       "17855         RT @powerfeen: I wish wealth upon me and all my friends so this can be our only problem  https://t.co/WUi2Fx8aKR   \n",
       "48687   @realdawnsummers @AnthonyCurtis68 i thought that was still legal in Mississippi \\n\\nkidding!!!!! lol... that's Alabama   \n",
       "60415                                      RT @itsaustonpugh: when i put \"\" this is what i really mean https://t.co/zPuraXzTLE   \n",
       "97450                                                                    RT @fine_ass_mari: I hate texting.. come live with me   \n",
       "13183                                                                         @HesterDressage @Hezzzza Wow what a lucky girl ️   \n",
       "131561                                                                                                I want to starve myself    \n",
       "62357                                                 RT @mytweetsaremid: Tupac really killed it here  https://t.co/Edt8dnDR7r   \n",
       "\n",
       "       true pred  \n",
       "44191     😭    🙏  \n",
       "58270     💕    😂  \n",
       "67486     🙌    🙏  \n",
       "17855     😭    😭  \n",
       "48687     🤔    👍  \n",
       "60415     👀    👀  \n",
       "97450     😂    😂  \n",
       "13183     ❤    ❤  \n",
       "131561    😂    😂  \n",
       "62357     🔥    😭  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 128\n",
    "inspect_tweets = test_tweets.sample(100)\n",
    "predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
    "show = pd.DataFrame({\n",
    "    'text': inspect_tweets['text'],\n",
    "    'true': inspect_tweets['emoji'],\n",
    "    'pred': [emojis[np.argmax(x)] for x in predicted],\n",
    "})\n",
    "show = show[['text', 'true', 'pred']]\n",
    "show.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_cnn_input (InputLayer)     [(None, 155, 1457)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 152, 128)     746112      <tensorflow.python.keras.engine.i\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 151, 128)     932608      <tensorflow.python.keras.engine.i\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 150, 128)     1119104     <tensorflow.python.keras.engine.i\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 38, 128)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 37, 128)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 37, 128)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 38, 128)      0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 37, 128)      0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 37, 128)      0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      131328      <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 33, 256)      164096      <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 32, 256)      196864      <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 8, 256)       0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 8, 256)       0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 8, 256)       0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8, 256)       0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8, 256)       0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 8, 256)       0           <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24, 256)      0           <tensorflow.python.keras.layers.c\n",
      "                                                                 <tensorflow.python.keras.layers.c\n",
      "                                                                 <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 24, 256)      0           <tensorflow.python.keras.layers.m\n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6144)         0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          786560      <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "char_cnn_predictions (Dense)    (None, 29)           3741        <tensorflow.python.keras.layers.c\n",
      "==================================================================================================\n",
      "Total params: 4,080,413\n",
      "Trainable params: 4,080,413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "def create_char_cnn_model2(num_chars, max_sequence_len, num_labels, drop_out=0.25):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (4, 5, 6):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "        dropout_1x = Dropout(drop_out)(max_pool_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "        dropout_2x = Dropout(drop_out)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    merged = Concatenate(axis=1)(layers)\n",
    "\n",
    "    dropout = Dropout(drop_out)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model2 = create_char_cnn_model2(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 319s - loss: 2.9711 - acc: 0.2252\n",
      "Epoch 2/30\n",
      " - 183s - loss: 2.8548 - acc: 0.2462\n",
      "Epoch 3/30\n",
      " - 185s - loss: 2.7502 - acc: 0.2732\n",
      "Epoch 4/30\n",
      " - 187s - loss: 2.5882 - acc: 0.3227\n",
      "Epoch 5/30\n",
      " - 187s - loss: 2.4524 - acc: 0.3555\n",
      "Epoch 6/30\n",
      " - 185s - loss: 2.3754 - acc: 0.3711\n",
      "Epoch 7/30\n",
      " - 186s - loss: 2.3008 - acc: 0.3874\n",
      "Epoch 8/30\n",
      " - 184s - loss: 2.2366 - acc: 0.4045\n",
      "Epoch 9/30\n",
      " - 189s - loss: 2.1685 - acc: 0.4236\n",
      "Epoch 10/30\n",
      " - 184s - loss: 2.0901 - acc: 0.4389\n",
      "Epoch 11/30\n",
      " - 187s - loss: 2.0251 - acc: 0.4585\n",
      "Epoch 12/30\n",
      " - 185s - loss: 1.9714 - acc: 0.4709\n",
      "Epoch 13/30\n",
      " - 185s - loss: 1.9300 - acc: 0.4819\n",
      "Epoch 14/30\n",
      " - 186s - loss: 1.8593 - acc: 0.4990\n",
      "Epoch 15/30\n",
      " - 186s - loss: 1.8430 - acc: 0.5057\n",
      "Epoch 16/30\n",
      " - 186s - loss: 1.7937 - acc: 0.5158\n",
      "Epoch 17/30\n",
      " - 187s - loss: 1.7518 - acc: 0.5251\n",
      "Epoch 18/30\n",
      " - 186s - loss: 1.7158 - acc: 0.5335\n",
      "Epoch 19/30\n",
      " - 185s - loss: 1.6895 - acc: 0.5404\n",
      "Epoch 20/30\n",
      " - 187s - loss: 1.6376 - acc: 0.5544\n",
      "Epoch 21/30\n",
      " - 186s - loss: 1.6285 - acc: 0.5560\n",
      "Epoch 22/30\n",
      " - 186s - loss: 1.6036 - acc: 0.5602\n",
      "Epoch 23/30\n",
      " - 187s - loss: 1.5507 - acc: 0.5733\n",
      "Epoch 24/30\n",
      " - 185s - loss: 1.5537 - acc: 0.5745\n",
      "Epoch 25/30\n",
      " - 186s - loss: 1.5066 - acc: 0.5860\n",
      "Epoch 26/30\n",
      " - 188s - loss: 1.4944 - acc: 0.5877\n",
      "Epoch 27/30\n",
      " - 185s - loss: 1.4642 - acc: 0.5950\n",
      "Epoch 28/30\n",
      " - 186s - loss: 1.4437 - acc: 0.6003\n",
      "Epoch 29/30\n",
      " - 185s - loss: 1.4359 - acc: 0.6033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8317e39ef0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "char_cnn_model2.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=30,\n",
    "    steps_per_epoch=len(train_tweets) // BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7012174129486084, 0.55358887]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model2.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75s - loss: 2.3855 - acc: 0.4368\\n[2.8089022636413574, 0.38840296648550726]'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"75s - loss: 2.3855 - acc: 0.4368\\n[2.8089022636413574, 0.38840296648550726]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and preparing our data\n",
    "\n",
    "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = tokenizer.texts_to_sequences(train_tweets['text'])\n",
    "test_tokens = tokenizer.texts_to_sequences(test_tweets['text'])\n",
    "max_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\n",
    "training_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\n",
    "test_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = np.asarray([emoji_to_idx[em] for em in train_tweets['emoji']])\n",
    "test_labels = np.asarray([emoji_to_idx[em] for em in test_tweets['emoji']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(train_tweets['text'], size=100, window=5, min_count=1, workers=4)\n",
    "w2v.save('data/twitter_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(tokenizer):\n",
    "    w2v_model = Word2Vec.load('data/twitter_w2v.model')\n",
    "    w2v = np.zeros((tokenizer.num_words, w2v_model.wv.syn0.shape[1]))\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v >= tokenizer.num_words:\n",
    "            continue\n",
    "        if k in w2v_model.wv:\n",
    "            w2v[v] = w2v_model.wv[k]\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 50000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This may take a while to load\n",
    "\n",
    "w2v = load_weights(tokenizer)\n",
    "\n",
    "model = Word2Vec.load('data/twitter_w2v.model')\n",
    "w2v = np.zeros((tokenizer.num_words, model.wv.syn0.shape[1]))\n",
    "found = 0\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v >= tokenizer.num_words:\n",
    "        continue\n",
    "    if k in model.wv:\n",
    "        w2v[v] = model.wv[k]\n",
    "        found += 1\n",
    "found, tokenizer.num_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Level CNN\n",
    "\n",
    "As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cnn_input (InputLayer)          [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cnn_embedding (Embedding)       (None, 44, 100)      5000000     <tensorflow.python.keras.engine.i\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 43, 128)      25728       <tensorflow.python.keras.layers.e\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 42, 128)      38528       <tensorflow.python.keras.layers.e\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 21, 128)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 21, 128)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 256)      65792       <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 19, 256)      98560       <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 10, 256)      0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 9, 256)       0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 9, 256)       131328      <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 7, 256)       196864      <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           <tensorflow.python.keras.layers.c\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           <tensorflow.python.keras.layers.p\n",
      "                                                                 <tensorflow.python.keras.layers.p\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       <tensorflow.python.keras.layers.m\n",
      "__________________________________________________________________________________________________\n",
      "cnn_predictions (Dense)         (None, 29)           3741        <tensorflow.python.keras.layers.c\n",
      "==================================================================================================\n",
      "Total params: 5,626,205\n",
      "Trainable params: 5,626,205\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None, drop_out=0.2):\n",
    "    message = Input(shape=(max_num_tokens,), dtype='int32', name='cnn_input')\n",
    "    \n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='cnn_embedding')(message)\n",
    "    \n",
    "    global_pools = []\n",
    "    for window in 2, 3:\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(embedding)\n",
    "        max_pool_1x = MaxPooling1D(2)(conv_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_1x)\n",
    "        max_pool_2x = MaxPooling1D(2)(conv_2x)\n",
    "        conv_3x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_2x)\n",
    "\n",
    "        global_pools.append(GlobalMaxPooling1D()(conv_3x))\n",
    "\n",
    "    merged = Concatenate(axis=1)(global_pools)\n",
    "    fc1 = Dense(units=128, activation='elu')(merged)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='cnn_predictions')(fc1)\n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3067: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.\n\t [[{{node training_2/RMSprop/RMSprop/update/ResourceSparseApplyRMSProp}}]] [Op:StatefulPartitionedCall]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bd30d4f73375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3165\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3167\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3168\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3169\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got two values for keyword '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyword arguments {} unknown.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    268\u001b[0m           \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m           executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[1;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m           executor_type=executor_type)\n\u001b[0m\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/tensorflow/python/ops/gen_functional_ops.py\u001b[0m in \u001b[0;36mstateful_partitioned_call\u001b[0;34m(args, Tout, f, config, config_proto, executor_type, name)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Could not colocate node with its resource and reference inputs; devices /job:localhost/replica:0/task:0/device:CPU:0 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.\n\t [[{{node training_2/RMSprop/RMSprop/update/ResourceSparseApplyRMSProp}}]] [Op:StatefulPartitionedCall]"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = Input(shape=(None,), dtype='int32', name='lstm_input')\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='lstm_embedding')(message)\n",
    "\n",
    "    lstm_1 = LSTM(units=128, return_sequences=False)(embedding)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='lstm_predictions')(lstm_1)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=12, batch_size=1024, callbacks=[early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models\n",
    "\n",
    "Let's compare the predictions from our models on a sample of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_char_vectors, _ = next(data_generator(test_tweets, None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    label: [emojis[np.argmax(x)] for x in pred]\n",
    "    for label, pred in (\n",
    "        ('lstm', lstm_model.predict(test_tokens[:100])),\n",
    "        ('char_cnn', char_cnn_model.predict(test_char_vectors[:100])),\n",
    "        ('cnn', cnn_model.predict(test_tokens[:100])),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe just for test data\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = test_tweets[:100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['text'],\n",
    "    'true': test_df['emoji'],\n",
    "    **predictions\n",
    "})\n",
    "eval_df[['content', 'true', 'char_cnn', 'cnn', 'lstm']].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data_generator(tweets, tokens, batch_size):\n",
    "    tweets = tweets.reset_index()\n",
    "    while True:\n",
    "        batch_idx = random.sample(range(len(tweets)), batch_size)\n",
    "        tweet_batch = tweets.iloc[batch_idx]\n",
    "        token_batch = tokens[batch_idx]\n",
    "        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        token_vec = np.zeros((batch_size, max_num_tokens))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (token_row, (_, tweet_row)) in enumerate(zip(token_batch, tweet_batch.iterrows())):\n",
    "            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n",
    "            for ch_idx, ch in enumerate(tweet_row['text']):\n",
    "                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "            token_vec[row_idx, :] = token_row\n",
    "        yield {'char_cnn_input': char_vec, 'cnn_input': token_vec, 'lstm_input': token_vec}, y\n",
    "\n",
    "d, y = next(combined_data_generator(train_tweets, training_tokens, 5))\n",
    "d['lstm_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_layer(model):\n",
    "    layers = [layer for layer in model.layers if layer.name.endswith('_predictions')]\n",
    "    return layers[0].output\n",
    "\n",
    "def create_ensemble(*models):\n",
    "    inputs = [model.input for model in models]\n",
    "    predictions = [prediction_layer(model) for model in models]\n",
    "    merged = Average()(predictions)\n",
    "    model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[merged],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "ensemble = create_ensemble(char_cnn_model2, cnn_model, lstm_model)\n",
    "ensemble.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "ensemble.fit_generator(\n",
    "    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.evaluate_generator(\n",
    "    combined_data_generator(test_tweets, test_tokens, BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
