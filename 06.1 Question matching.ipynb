{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers, models, utils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_everything():\n",
    "    import tensorflow as tf\n",
    "    %reset -f in out dhist\n",
    "    tf.reset_default_graph()\n",
    "    K.set_session(tf.InteractiveSession())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for our networks.  We keep these deliberately small to reduce training time.\n",
    "\n",
    "VOCAB_SIZE = 250000\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_DOC_LEN = 128\n",
    "MIN_DOC_LEN = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "91000/1000000"
     ]
    }
   ],
   "source": [
    "def extract_stackexchange(filename, limit=1000000):\n",
    "    json_file = filename + 'limit=%s.json' % limit\n",
    "\n",
    "    rows = []\n",
    "    for i, line in enumerate(os.popen('7z x -so \"%s\" Posts.xml' % filename)):\n",
    "        line = str(line)\n",
    "        if not line.startswith('  <row'):\n",
    "            continue\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('\\r%05d/%05d' % (i, limit), end='', flush=True)\n",
    "\n",
    "        parts = line[6:-5].split('\"')\n",
    "        record = {}\n",
    "        for i in range(0, len(parts), 2):\n",
    "            k = parts[i].replace('=', '').strip()\n",
    "            v = parts[i+1].strip()\n",
    "            record[k] = v\n",
    "        rows.append(record)\n",
    "        \n",
    "        if len(rows) > limit:\n",
    "            break\n",
    "    \n",
    "    with open(json_file, 'w') as fout:\n",
    "        json.dump(rows, fout)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "xml_7z = utils.get_file(\n",
    "    fname='travel.stackexchange.com.7z',\n",
    "    origin='https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z',\n",
    ")\n",
    "print()\n",
    "\n",
    "rows = extract_stackexchange(xml_7z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Now that we have extracted our data, let's clean it up and take a look at what we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>...</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Title</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp;lt;p&amp;gt;My fiancée and I are looking for a go...</td>\n",
       "      <td>2013-02-25T23:52:47.953</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:19:34.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-24T14:52:14.760</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;caribbean&amp;gt;&amp;lt;cruising&amp;gt;&amp;lt;vacations...</td>\n",
       "      <td>What are some Caribbean cruises for October?</td>\n",
       "      <td>444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;p&amp;gt;This was one of our definition questi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:22:33.760</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-08-26T00:04:13.520</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>&amp;lt;guides&amp;gt;&amp;lt;extreme-tourism&amp;gt;&amp;lt;amazo...</td>\n",
       "      <td>How can I find a guide that will take me safel...</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;lt;p&amp;gt;One way would be to go through an Adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:24:28.080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-06-21T20:24:28.080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;lt;p&amp;gt;Singapore Airlines has an all-busines...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:24:57.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-09T09:55:22.743</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;loyalty-programs&amp;gt;&amp;lt;routes&amp;gt;&amp;lt;ewr&amp;...</td>\n",
       "      <td>Does Singapore Airlines offer any reward seats...</td>\n",
       "      <td>253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>770</td>\n",
       "      <td>5</td>\n",
       "      <td>&amp;lt;p&amp;gt;Another definition question that inte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:25:56.787</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-10-12T20:49:08.110</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>&amp;lt;romania&amp;gt;&amp;lt;transportation&amp;gt;</td>\n",
       "      <td>What is the easiest transportation to use thro...</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId AnswerCount  \\\n",
       "Id                                \n",
       "1               393           4   \n",
       "2               NaN           8   \n",
       "3               NaN         NaN   \n",
       "4               NaN           1   \n",
       "5               770           5   \n",
       "\n",
       "                                                 Body  \\\n",
       "Id                                                      \n",
       "1   &lt;p&gt;My fiancée and I are looking for a go...   \n",
       "2   &lt;p&gt;This was one of our definition questi...   \n",
       "3   &lt;p&gt;One way would be to go through an Adv...   \n",
       "4   &lt;p&gt;Singapore Airlines has an all-busines...   \n",
       "5   &lt;p&gt;Another definition question that inte...   \n",
       "\n",
       "                 ClosedDate CommentCount CommunityOwnedDate  \\\n",
       "Id                                                            \n",
       "1   2013-02-25T23:52:47.953            4                NaN   \n",
       "2                       NaN            4                NaN   \n",
       "3                       NaN            2                NaN   \n",
       "4                       NaN            1                NaN   \n",
       "5                       NaN            0                NaN   \n",
       "\n",
       "               CreationDate FavoriteCount  Id         LastActivityDate  \\\n",
       "Id                                                                       \n",
       "1   2011-06-21T20:19:34.730           NaN   1  2012-05-24T14:52:14.760   \n",
       "2   2011-06-21T20:22:33.760             5   2  2018-08-26T00:04:13.520   \n",
       "3   2011-06-21T20:24:28.080           NaN   3  2011-06-21T20:24:28.080   \n",
       "4   2011-06-21T20:24:57.160           NaN   4  2013-01-09T09:55:22.743   \n",
       "5   2011-06-21T20:25:56.787             2   5  2012-10-12T20:49:08.110   \n",
       "\n",
       "      ...    LastEditorDisplayName LastEditorUserId OwnerDisplayName  \\\n",
       "Id    ...                                                              \n",
       "1     ...                      NaN              101              NaN   \n",
       "2     ...                      NaN            51577              NaN   \n",
       "3     ...                      NaN              NaN              NaN   \n",
       "4     ...                      NaN              693              NaN   \n",
       "5     ...                      NaN              101              NaN   \n",
       "\n",
       "   OwnerUserId ParentId PostTypeId  Score  \\\n",
       "Id                                          \n",
       "1            9      NaN          1      8   \n",
       "2           13      NaN          1     36   \n",
       "3            9        2          2     14   \n",
       "4           24      NaN          1      8   \n",
       "5           13      NaN          1     13   \n",
       "\n",
       "                                                 Tags  \\\n",
       "Id                                                      \n",
       "1   &lt;caribbean&gt;&lt;cruising&gt;&lt;vacations...   \n",
       "2   &lt;guides&gt;&lt;extreme-tourism&gt;&lt;amazo...   \n",
       "3                                                       \n",
       "4   &lt;loyalty-programs&gt;&lt;routes&gt;&lt;ewr&...   \n",
       "5               &lt;romania&gt;&lt;transportation&gt;   \n",
       "\n",
       "                                                Title ViewCount  \n",
       "Id                                                               \n",
       "1        What are some Caribbean cruises for October?     444.0  \n",
       "2   How can I find a guide that will take me safel...    1997.0  \n",
       "3                                                           NaN  \n",
       "4   Does Singapore Airlines offer any reward seats...     253.0  \n",
       "5   What is the easiest transportation to use thro...     421.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(rows)    \n",
    "df = df.set_index('Id', drop=False)\n",
    "df['Title'] = df['Title'].fillna('').astype('str')\n",
    "df['Tags'] = df['Tags'].fillna('').astype('str')\n",
    "df['Body'] = df['Body'].fillna('').astype('str')\n",
    "df['Id'] = df['Id'].astype('int')\n",
    "df['PostTypeId'] = df['PostTypeId'].astype('int')\n",
    "df['ViewCount'] = df['ViewCount'].astype('float')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do I need a US visa to transit (or layover) through an American airport?',\n",
       " 'How to get from Nice to Monaco by public transport?',\n",
       " 'Should my first trip be to the country which issued my Schengen Visa?',\n",
       " 'Can I use Google Maps traffic information to estimate driving time for a specific date/time?',\n",
       " 'Are aerosol cans allowed and safe, in checked luggage?',\n",
       " 'How to track my UK Visa Application Status?',\n",
       " \"When applying for an Indian Passport, how do I know if I'm in the ECR or non-ECR category?\",\n",
       " 'Are battery packs allowed in hand luggage?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['ViewCount'] > 250000]['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['Body'] + df['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF/IDF Values\n",
    "\n",
    "total_count = sum(tokenizer.word_counts.values())\n",
    "idf = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained word2vec embeddings\n",
    "\n",
    "import gensim\n",
    "\n",
    "glove_100d = utils.get_file(\n",
    "    fname='glove.6B.100d.txt',\n",
    "    origin='https://storage.googleapis.com/deep-learning-cookbook/glove.6B.100d.txt',\n",
    ")\n",
    "\n",
    "w2v_100d = glove_100d + '.w2v'\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_100d, w2v_100d)\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_100d)\n",
    "\n",
    "w2v_weights = np.zeros((VOCAB_SIZE, w2v_model.syn0.shape[1]))\n",
    "idf_weights = np.zeros((VOCAB_SIZE, 1))\n",
    "\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v >= VOCAB_SIZE:\n",
    "        continue\n",
    "    \n",
    "    if k in w2v_model:\n",
    "        w2v_weights[v] = w2v_model[k]\n",
    "    \n",
    "    idf_weights[v] = idf[k]\n",
    "    \n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens'] = tokenizer.texts_to_sequences(df['Title'])\n",
    "df['body_tokens'] = tokenizer.texts_to_sequences(df['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# We can create a data generator that will randomly title and body tokens for questions.  We'll use random text\n",
    "# from other questions as a negative example when necessary.\n",
    "def data_generator(batch_size, negative_samples=1):\n",
    "    questions = df[df['PostTypeId'] == 1]\n",
    "    all_q_ids = list(questions.index)\n",
    "        \n",
    "    batch_x_a = []\n",
    "    batch_x_b = []\n",
    "    batch_y = []\n",
    "    \n",
    "    def _add(x_a, x_b, y):\n",
    "        batch_x_a.append(x_a[:MAX_DOC_LEN])\n",
    "        batch_x_b.append(x_b[:MAX_DOC_LEN])\n",
    "        batch_y.append(y)\n",
    "    \n",
    "    while True:\n",
    "        questions = questions.sample(frac=1.0)\n",
    "        \n",
    "        for i, q in questions.iterrows():\n",
    "            _add(q['title_tokens'], q['body_tokens'], 1)\n",
    "            \n",
    "            negative_q = random.sample(all_q_ids, negative_samples)\n",
    "            for nq_id in negative_q:\n",
    "                _add(q['title_tokens'], df.at[nq_id, 'body_tokens'], 0)            \n",
    "            \n",
    "            if len(batch_y) >= batch_size:\n",
    "                yield ({\n",
    "                    'title': pad_sequences(batch_x_a, maxlen=None),\n",
    "                    'body': pad_sequences(batch_x_b, maxlen=None),\n",
    "                }, np.asarray(batch_y))\n",
    "                \n",
    "                batch_x_a = []\n",
    "                batch_x_b = []\n",
    "                batch_y = []\n",
    "\n",
    "# dg = data_generator(1, 2)\n",
    "# next(dg)\n",
    "# next(dg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Lookups\n",
    "\n",
    "Let's define a helper class for looking up our embedding results.  We'll use it\n",
    "to verify our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df[df['PostTypeId'] == 1]['Title'].reset_index(drop=True)\n",
    "question_tokens = pad_sequences(tokenizer.texts_to_sequences(questions))\n",
    "\n",
    "class EmbeddingWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        self._r = questions\n",
    "        self._i = {i:s for (i, s) in enumerate(questions)}\n",
    "        self._w = model.predict({'title': question_tokens}, verbose=1, batch_size=1024)\n",
    "        self._model = model\n",
    "        self._norm = np.sqrt(np.sum(self._w * self._w + 1e-5, axis=1))\n",
    "\n",
    "    def nearest(self, sentence, n=10):\n",
    "        x = tokenizer.texts_to_sequences([sentence])\n",
    "        if len(x[0]) < MIN_DOC_LEN:\n",
    "            x[0] += [0] * (MIN_DOC_LEN - len(x))\n",
    "        e = self._model.predict(np.asarray(x))[0]\n",
    "        norm_e = np.sqrt(np.dot(e, e))\n",
    "        dist = np.dot(self._w, e) / (norm_e * self._norm)\n",
    "\n",
    "        top_idx = np.argsort(dist)[-n:]\n",
    "        return pd.DataFrame.from_records([\n",
    "            {'question': self._r[i], 'dist': float(dist[i])}\n",
    "            for i in top_idx\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first model will just sum up the embeddings of each token.\n",
    "# The similarity between documents will be the dot product of the final embedding.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def sum_model(embedding_size, vocab_size, embedding_weights=None, idf_weights=None):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    def make_embedding(name):\n",
    "        if embedding_weights is not None:\n",
    "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=w2v_weights.shape[1], \n",
    "                                         weights=[w2v_weights], trainable=False, \n",
    "                                         name='%s/embedding' % name)\n",
    "        else:\n",
    "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                        name='%s/embedding' % name)\n",
    "\n",
    "        if idf_weights is not None:\n",
    "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1, \n",
    "                                   weights=[idf_weights], trainable=False,\n",
    "                                   name='%s/idf' % name)\n",
    "        else:\n",
    "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1,\n",
    "                                   name='%s/idf' % name)\n",
    "            \n",
    "        return embedding, idf\n",
    "    \n",
    "    embedding_a, idf_a = make_embedding('a')\n",
    "    embedding_b, idf_b = embedding_a, idf_a\n",
    "#     embedding_b, idf_b = make_embedding('b')\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        [embedding, idf] = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "\n",
    "    sum_a = sum_layer([mask(embedding_a(title)), idf_a(title)])\n",
    "    sum_b = sum_layer([mask(embedding_b(body)), idf_b(body)])\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    sim_model.summary()\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1364: calling reduce_any (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3032: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "a/embedding (Embedding)         (None, None, 100)    25000000    title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           a/embedding[0][0]                \n",
      "                                                                 a/embedding[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "a/idf (Embedding)               (None, None, 1)      250000      title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 100)          0           masking_1[0][0]                  \n",
      "                                                                 a/idf[0][0]                      \n",
      "                                                                 masking_1[1][0]                  \n",
      "                                                                 a/idf[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           combine_and_sum[0][0]            \n",
      "                                                                 combine_and_sum[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,250,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 25,250,000\n",
      "__________________________________________________________________________________________________\n",
      "4096/4096 [==============================] - 0s 77us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9587535602040589, 0.5107421875]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try using our model with pretrained weights from word2vec\n",
    "\n",
    "sum_model_precomputed, sum_embedding_precomputed = sum_model(\n",
    "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE,\n",
    "    embedding_weights=w2v_weights, idf_weights=idf_weights\n",
    ")\n",
    "\n",
    "x, y = next(data_generator(batch_size=4096))\n",
    "sum_model_precomputed.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33413/33413 [==============================] - 0s 2us/step\n",
      "Roundtrip ticket versus one way\n",
      "Shinkansen from Kyoto to Hiroshima\n",
      "Bus tour of Germany\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>question</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.811748</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Buy a roundtrip ticket for two people, but second person only travels on return - is that possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.813634</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>How to pick the (phony) return destination for a roundtrip ticket intended as a one-way?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.815370</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>What is cheapest way to fly around SE Asia in a circuit - hub with roundtrip tickets or sequence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.826476</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>The penalty for changing an airline ticket is per leg or per ticket?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.753331</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Culture Day in Osaka/Kyoto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756834</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Where does the Tokaido Shinkansen stop in Tokyo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.775567</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Best connection Tokyo - Kyoto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.812919</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Travel from Tokyo to Sendai with Shinkansen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.891266</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Trip in the south of Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.895140</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Travelling outside of Germany on a German Working Holiday visa (Australian)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.898387</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Round trip in Netherlands, Belgium and Germany with a car and a tent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.902455</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Transport for Tour of Southern Spain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist                            question  \\\n",
       "0  0.811748     Roundtrip ticket versus one way   \n",
       "1  0.813634     Roundtrip ticket versus one way   \n",
       "2  0.815370     Roundtrip ticket versus one way   \n",
       "3  0.826476     Roundtrip ticket versus one way   \n",
       "0  0.753331  Shinkansen from Kyoto to Hiroshima   \n",
       "1  0.756834  Shinkansen from Kyoto to Hiroshima   \n",
       "2  0.775567  Shinkansen from Kyoto to Hiroshima   \n",
       "3  0.812919  Shinkansen from Kyoto to Hiroshima   \n",
       "0  0.891266                 Bus tour of Germany   \n",
       "1  0.895140                 Bus tour of Germany   \n",
       "2  0.898387                 Bus tour of Germany   \n",
       "3  0.902455                 Bus tour of Germany   \n",
       "\n",
       "                                                                                                result  \n",
       "0   Buy a roundtrip ticket for two people, but second person only travels on return - is that possible  \n",
       "1             How to pick the (phony) return destination for a roundtrip ticket intended as a one-way?  \n",
       "2  What is cheapest way to fly around SE Asia in a circuit - hub with roundtrip tickets or sequence...  \n",
       "3                                 The penalty for changing an airline ticket is per leg or per ticket?  \n",
       "0                                                                           Culture Day in Osaka/Kyoto  \n",
       "1                                                     Where does the Tokaido Shinkansen stop in Tokyo?  \n",
       "2                                                                        Best connection Tokyo - Kyoto  \n",
       "3                                                          Travel from Tokyo to Sendai with Shinkansen  \n",
       "0                                                                         Trip in the south of Germany  \n",
       "1                          Travelling outside of Germany on a German Working Holiday visa (Australian)  \n",
       "2                                 Round trip in Netherlands, Belgium and Germany with a car and a tent  \n",
       "3                                                                 Transport for Tour of Southern Spain  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_QUESTIONS = [\n",
    "    'Roundtrip ticket versus one way',\n",
    "    'Shinkansen from Kyoto to Hiroshima',\n",
    "    'Bus tour of Germany',\n",
    "]\n",
    "\n",
    "def evaluate_sample(lookup):\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    results = []\n",
    "    for q in SAMPLE_QUESTIONS:\n",
    "        print(q)\n",
    "        q_res = lookup.nearest(q, n=4)\n",
    "        q_res['result'] = q_res['question']\n",
    "        q_res['question'] = q\n",
    "        results.append(q_res)\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "lookup = EmbeddingWrapper(model=sum_embedding_precomputed)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own network\n",
    "\n",
    "The results are okay but not great... instead of using the word2vec embeddings, what happens if we train our network end-to-end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "a/embedding (Embedding)         (None, None, 100)    25000000    title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 100)    0           a/embedding[0][0]                \n",
      "                                                                 a/embedding[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "a/idf (Embedding)               (None, None, 1)      250000      title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 100)          0           masking_2[0][0]                  \n",
      "                                                                 a/idf[0][0]                      \n",
      "                                                                 masking_2[1][0]                  \n",
      "                                                                 a/idf[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           combine_and_sum[0][0]            \n",
      "                                                                 combine_and_sum[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,250,000\n",
      "Trainable params: 25,250,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 44s 44ms/step - loss: 0.2666 - acc: 0.9186\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1459 - acc: 0.9759\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1187 - acc: 0.9833\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.1056 - acc: 0.9866\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0968 - acc: 0.9885\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0911 - acc: 0.9894\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0868 - acc: 0.9904\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0827 - acc: 0.9911\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 42s 42ms/step - loss: 0.0803 - acc: 0.9914\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 43s 43ms/step - loss: 0.0776 - acc: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5171a789e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_model_trained, sum_embedding_trained = sum_model(\n",
    "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE, \n",
    "    embedding_weights=None,\n",
    "    idf_weights=None\n",
    ")\n",
    "sum_model_trained.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33413/33413 [==============================] - 0s 3us/step\n",
      "Roundtrip ticket versus one way\n",
      "Shinkansen from Kyoto to Hiroshima\n",
      "Bus tour of Germany\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>question</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.759106</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Is it necessary to book two separate tickets if one passengers will deliberately miss the return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767955</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>book return ticket with additional traveller on return ticket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.768983</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>How to get return prices for a one way ticket?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.869606</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Buy a roundtrip ticket for two people, but second person only travels on return - is that possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963508</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>How early should I reserve Shinkansen tickets during April?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.970527</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Can I buy Shinkansen tickets that allow onward travel on JR regional trains?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972454</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Can I converse with Japanese travellers on the Shinkansen or is it rude?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973669</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>What are my options for reserving JR Shinkansen tickets in advance over the new year period?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.618510</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>How to find inter-country buses in Europe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.620923</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Where can I get a visa for Germany?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.721527</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Searching for internal bus connections in Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.751947</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>European bus tour companies for middle age people?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist                            question  \\\n",
       "0  0.759106     Roundtrip ticket versus one way   \n",
       "1  0.767955     Roundtrip ticket versus one way   \n",
       "2  0.768983     Roundtrip ticket versus one way   \n",
       "3  0.869606     Roundtrip ticket versus one way   \n",
       "0  0.963508  Shinkansen from Kyoto to Hiroshima   \n",
       "1  0.970527  Shinkansen from Kyoto to Hiroshima   \n",
       "2  0.972454  Shinkansen from Kyoto to Hiroshima   \n",
       "3  0.973669  Shinkansen from Kyoto to Hiroshima   \n",
       "0  0.618510                 Bus tour of Germany   \n",
       "1  0.620923                 Bus tour of Germany   \n",
       "2  0.721527                 Bus tour of Germany   \n",
       "3  0.751947                 Bus tour of Germany   \n",
       "\n",
       "                                                                                                result  \n",
       "0  Is it necessary to book two separate tickets if one passengers will deliberately miss the return...  \n",
       "1                                        book return ticket with additional traveller on return ticket  \n",
       "2                                                       How to get return prices for a one way ticket?  \n",
       "3   Buy a roundtrip ticket for two people, but second person only travels on return - is that possible  \n",
       "0                                          How early should I reserve Shinkansen tickets during April?  \n",
       "1                         Can I buy Shinkansen tickets that allow onward travel on JR regional trains?  \n",
       "2                             Can I converse with Japanese travellers on the Shinkansen or is it rude?  \n",
       "3         What are my options for reserving JR Shinkansen tickets in advance over the new year period?  \n",
       "0                                                           How to find inter-country buses in Europe?  \n",
       "1                                                                  Where can I get a visa for Germany?  \n",
       "2                                                    Searching for internal bus connections in Germany  \n",
       "3                                                   European bus tour companies for middle age people?  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = EmbeddingWrapper(model=sum_embedding_trained)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "Using a sum-of-embeddings model works well. What happens if we try to make a simple CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(embedding_size, vocab_size):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    embedding = layers.Embedding(\n",
    "        mask_zero=False,\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(256, 3)\n",
    "    cnn_2 = layers.Convolution1D(256, 3)\n",
    "    cnn_3 = layers.Convolution1D(256, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=2, pool_size=3)\n",
    "\n",
    "    def forward(input):\n",
    "        embed = embedding(input)\n",
    "        return global_pool(\n",
    "            cnn_2(local_pool(cnn_1(embed))))\n",
    "\n",
    "    sum_a = forward(title)\n",
    "    sum_b = forward(body)\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=False)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 25)     6250000     title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    19456       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 256)    0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    196864      max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_1[1][0]     \n",
      "==================================================================================================\n",
      "Total params: 6,466,320\n",
      "Trainable params: 6,466,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.6428 - acc: 0.6386\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5672 - acc: 0.7505\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5323 - acc: 0.7852\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5206 - acc: 0.7859\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5158 - acc: 0.7604\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.5086 - acc: 0.7542\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.4926 - acc: 0.7224\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5253 - acc: 0.6916\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5492 - acc: 0.6444\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 0.5233 - acc: 0.6226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f516e6cf0f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn, cnn_embedding = cnn_model(embedding_size=25, vocab_size=VOCAB_SIZE)\n",
    "cnn.summary()\n",
    "cnn.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33413/33413 [==============================] - 0s 10us/step\n",
      "Roundtrip ticket versus one way\n",
      "Shinkansen from Kyoto to Hiroshima\n",
      "Bus tour of Germany\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>question</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973556</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Short Connection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973795</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Schengeni Allamok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974133</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Contiki Alternative?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974888</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Tegelbergbahn Tickets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980363</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Mugging 'Etiquette'?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981248</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Schengeni Allamok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.981393</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Tegelbergbahn Tickets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981756</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Contiki Alternative?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977773</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Mugging 'Etiquette'?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.977812</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Tegelbergbahn Tickets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978095</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Contiki Alternative?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.978270</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Schengeni Allamok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist                            question                 result\n",
       "0  0.973556     Roundtrip ticket versus one way       Short Connection\n",
       "1  0.973795     Roundtrip ticket versus one way      Schengeni Allamok\n",
       "2  0.974133     Roundtrip ticket versus one way   Contiki Alternative?\n",
       "3  0.974888     Roundtrip ticket versus one way  Tegelbergbahn Tickets\n",
       "0  0.980363  Shinkansen from Kyoto to Hiroshima   Mugging 'Etiquette'?\n",
       "1  0.981248  Shinkansen from Kyoto to Hiroshima      Schengeni Allamok\n",
       "2  0.981393  Shinkansen from Kyoto to Hiroshima  Tegelbergbahn Tickets\n",
       "3  0.981756  Shinkansen from Kyoto to Hiroshima   Contiki Alternative?\n",
       "0  0.977773                 Bus tour of Germany   Mugging 'Etiquette'?\n",
       "1  0.977812                 Bus tour of Germany  Tegelbergbahn Tickets\n",
       "2  0.978095                 Bus tour of Germany   Contiki Alternative?\n",
       "3  0.978270                 Bus tour of Germany      Schengeni Allamok"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = EmbeddingWrapper(model=cnn_embedding)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "We can also make an LSTM model.  Warning, this will be very slow to train and evaluate unless you have a relatively fast GPU to run it on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(embedding_size, vocab_size):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    embedding = layers.Embedding(\n",
    "        mask_zero=True,\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "#         weights=[w2v_weights],\n",
    "#         trainable=False\n",
    "    )\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=512, return_sequences=True)\n",
    "    lstm_2 = layers.LSTM(units=512, return_sequences=False)\n",
    "    \n",
    "    sum_a = lstm_2(lstm_1(embedding(title)))\n",
    "    sum_b = lstm_2(lstm_1(embedding(body)))\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
    "#     sim = layers.Activation(activation='sigmoid')(sim)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    25000000    title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 512)    1255424     embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          2099200     lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 28,354,624\n",
      "Trainable params: 28,354,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 0.8176\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 0.8172\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 2.9111\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 8.0590\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 8.0590\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 8.0590\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 50s 500ms/step - loss: 8.0590\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 8.0590\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 8.0590\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 8.0590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f515e8b9c88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm, lstm_embedding = lstm_model(embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE)\n",
    "lstm.summary()\n",
    "lstm.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33413/33413 [==============================] - 5s 153us/step\n",
      "Roundtrip ticket versus one way\n",
      "Shinkansen from Kyoto to Hiroshima\n",
      "Bus tour of Germany\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>question</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985584</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Bank statement validity for UK Visa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.985713</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Entry for tourist with student visa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.985799</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Canadian visa rules for flight connections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985826</td>\n",
       "      <td>Roundtrip ticket versus one way</td>\n",
       "      <td>Is transit visa required for Turkey?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984077</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>What do tibetan door tassels symbolise?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.984214</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Cheap way to get to Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984664</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>Using the Vatican City train station?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985290</td>\n",
       "      <td>Shinkansen from Kyoto to Hiroshima</td>\n",
       "      <td>When does Allegiant Air add flights?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.971988</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Abandoned theme parks in Iceland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972762</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Renting camping equipment in Nambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972892</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Renting a flat in Montenegro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974609</td>\n",
       "      <td>Bus tour of Germany</td>\n",
       "      <td>Check-in of hand luggage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist                            question  \\\n",
       "0  0.985584     Roundtrip ticket versus one way   \n",
       "1  0.985713     Roundtrip ticket versus one way   \n",
       "2  0.985799     Roundtrip ticket versus one way   \n",
       "3  0.985826     Roundtrip ticket versus one way   \n",
       "0  0.984077  Shinkansen from Kyoto to Hiroshima   \n",
       "1  0.984214  Shinkansen from Kyoto to Hiroshima   \n",
       "2  0.984664  Shinkansen from Kyoto to Hiroshima   \n",
       "3  0.985290  Shinkansen from Kyoto to Hiroshima   \n",
       "0  0.971988                 Bus tour of Germany   \n",
       "1  0.972762                 Bus tour of Germany   \n",
       "2  0.972892                 Bus tour of Germany   \n",
       "3  0.974609                 Bus tour of Germany   \n",
       "\n",
       "                                       result  \n",
       "0         Bank statement validity for UK Visa  \n",
       "1         Entry for tourist with student visa  \n",
       "2  Canadian visa rules for flight connections  \n",
       "3        Is transit visa required for Turkey?  \n",
       "0     What do tibetan door tassels symbolise?  \n",
       "1               Cheap way to get to Argentina  \n",
       "2       Using the Vatican City train station?  \n",
       "3        When does Allegiant Air add flights?  \n",
       "0            Abandoned theme parks in Iceland  \n",
       "1         Renting camping equipment in Nambia  \n",
       "2                Renting a flat in Montenegro  \n",
       "3                    Check-in of hand luggage  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = EmbeddingWrapper(model=lstm_embedding)\n",
    "evaluate_sample(lookup)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
