{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "import xml.sax\n",
    "from sklearn import svm\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape\n",
    "from keras.layers.merge import Dot\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = requests.get('https://dumps.wikimedia.org/enwiki/').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_index = BeautifulSoup(index, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumps = [a['href'] for a in soup_index.find_all('a') \n",
    "             if a.has_attr('href') and a.text[:-1].isdigit()]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dump_url in sorted(dumps, reverse=True):\n",
    "  print(dump_url)\n",
    "  dump_html = index = requests.get('https://dumps.wikimedia.org/enwiki/' + dump_url).text\n",
    "  soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "  pages_xml = [a['href'] for a in soup_dump.find_all('a') \n",
    "             if a.has_attr('href') and a['href'].endswith('-pages-articles.xml.bz2')]\n",
    "  if pages_xml:\n",
    "    break\n",
    "  time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedia_dump = pages_xml[0].rsplit('/')[-1]\n",
    "if not os.path.isfile(wikipedia_dump):\n",
    "    url = 'https://dumps.wikimedia.org/' + pages_xml[0]\n",
    "    urlretrieve(url, wikipedia_dump)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_article(title, text):\n",
    "  rotten = [(re.findall('\\d\\d?\\d?%', p), re.findall('\\d\\.\\d\\/\\d+|$', p), p.lower().find('rotten tomatoes')) for p in text.split('\\n\\n')]\n",
    "  rating = next(((perc[0], rating[0]) for perc, rating, idx in rotten if len(perc) == 1 and idx > -1), (None, None))\n",
    "  wikicode = mwparserfromhell.parse(text)\n",
    "  film = next((template for template in wikicode.filter_templates() \n",
    "               if template.name.strip().lower() == 'infobox film'), None)\n",
    "  if film:\n",
    "    properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                  for param in film.params\n",
    "                  if param.value.strip_code().strip()\n",
    "                 }\n",
    "    links = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "    return (title, properties, links) + rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "  def __init__(self):\n",
    "    xml.sax.handler.ContentHandler.__init__(self)\n",
    "    self._buffer = None\n",
    "    self._values = {}\n",
    "    self._movies = []\n",
    "    self._curent_tag = None\n",
    "\n",
    "  def characters(self, content):\n",
    "    if self._curent_tag:\n",
    "      self._buffer.append(content)\n",
    "\n",
    "  def startElement(self, name, attrs):\n",
    "    if name in ('title', 'text'):\n",
    "      self._curent_tag = name\n",
    "      self._buffer = []\n",
    "\n",
    "  def endElement(self, name):\n",
    "    if name == self._curent_tag:\n",
    "      self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "    if name == 'page':\n",
    "      movie = process_article(**self._values)\n",
    "      if movie:\n",
    "        self._movies.append(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "for line in subprocess.Popen(['bzcat'], stdin=open(wikipedia_dump), stdout=subprocess.PIPE).stdout:\n",
    "  try:\n",
    "    parser.feed(line)\n",
    "  except StopIteration:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wp_movies.ndjson', 'wt') as fout:\n",
    "  for movie in handler._movies:\n",
    "    fout.write(json.dumps(movie) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
