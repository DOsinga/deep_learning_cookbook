{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a simple learner\n",
    "\n",
    "Before we try to build our deep learning models, let's make sure we can learn something using a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import regularizers\n",
    "import nb_utils\n",
    "\n",
    "emotion_csv = get_file('text_emotion.csv', \n",
    "                       'https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv')\n",
    "emotion_df = pd.read_csv(emotion_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X = tfidf_vec.fit_transform(emotion_df['content'])\n",
    "y = label_encoder.fit_transform(emotion_df['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2802272727272727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)\n",
    "predictions = bayes.predict(X_test)\n",
    "precision_score(predictions, y_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd 0.32856060606060605\n",
      "svm 0.21863636363636363\n",
      "random_forest 0.2821212121212121\n"
     ]
    }
   ],
   "source": [
    "classifiers = {'sgd': SGDClassifier(loss='hinge'),\n",
    "               'svm': SVC(),\n",
    "               'random_forest': RandomForestClassifier()}\n",
    "\n",
    "for lbl, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(lbl, precision_score(predictions, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what our model learned\n",
    "\n",
    "Our linear models appear to be learning something more powerful than \"pick the most popular category\".  We can take a quick look at which words they find the most correlated with each category before moving on to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import eye\n",
    "d = eye(len(tfidf_vec.vocabulary_))\n",
    "word_pred = bayes.predict_proba(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {v: k for k, v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "by_cls = defaultdict(Counter)\n",
    "for word_idx, pred in enumerate(word_pred):\n",
    "    for class_idx, score in enumerate(pred):\n",
    "        cls = label_encoder.classes_[class_idx]\n",
    "        by_cls[cls][inverse_vocab[word_idx]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger : confuzzled fridaaaayyyyy aaaaaaaaaaa transtelecom filthy\n",
      "boredom : squeaking ouuut cleanin sooooooo candyland3\n",
      "empty : _cheshire_cat_ bethsybsb conversating kimbermuffin less_than_3\n",
      "enthusiasm : lena_distractia foolproofdiva attending krisswouldhowse tatt\n",
      "fun : xbox bamboozle sanctuary oldies toodaayy\n",
      "happiness : excited woohoo excellent yay wars\n",
      "hate : hate hates suck fucking zomberellamcfox\n",
      "love : love mothers mommies moms loved\n",
      "neutral : www painting souljaboytellem link frenchieb\n",
      "relief : finally relax mastered relief inspiration\n",
      "sadness : sad sadly cry cried miss\n",
      "surprise : surprise wow surprised wtf surprisingly\n",
      "worry : worried poor throat hurts sick\n"
     ]
    }
   ],
   "source": [
    "for k in by_cls:\n",
    "    words = [x[0] for x in by_cls[k].most_common(5)]\n",
    "    print(k, ':', ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a deep model\n",
    "\n",
    "Now that we've seen how well a simple linear model can do, let's see if we can do any better with a deep learning model.  In this case, we don't have an excessive amount of training data: this constrains the models we can train effectively: use too big of a model, and we'll end up overfitting our data.\n",
    "\n",
    "We'll start with a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 167, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "chars = list(sorted(set(chain(*emotion_df['content']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in emotion_df['content'])\n",
    "\n",
    "char_vectors = []\n",
    "for txt in emotion_df['content']:\n",
    "    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n",
    "    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n",
    "    char_vectors.append(vec)\n",
    "char_vectors = np.asarray(char_vectors)\n",
    "char_vectors = pad_sequences(char_vectors)\n",
    "labels = label_encoder.transform(emotion_df['sentiment'])\n",
    "\n",
    "\n",
    "def split(lst):\n",
    "    training_count = int(0.9 * len(char_vectors))\n",
    "    return lst[:training_count], lst[training_count:]\n",
    "\n",
    "training_char_vectors, test_char_vectors = split(char_vectors)\n",
    "training_labels, test_labels = split(labels)\n",
    "\n",
    "char_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 167, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 162, 128)          76928     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 22, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 373,901\n",
      "Trainable params: 373,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(6)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(6)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36000/36000 [==============================] - 42s 1ms/step - loss: 3.2492 - acc: 0.2375\n",
      "Epoch 2/20\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 2.3864 - acc: 0.2462\n",
      "Epoch 3/20\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 2.1989 - acc: 0.2463\n",
      "Epoch 4/20\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 2.1564 - acc: 0.2488\n",
      "Epoch 5/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 2.1274 - acc: 0.2585\n",
      "Epoch 6/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 2.0944 - acc: 0.2758\n",
      "Epoch 7/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 2.0767 - acc: 0.2859\n",
      "Epoch 8/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 2.0546 - acc: 0.2959\n",
      "Epoch 9/20\n",
      "36000/36000 [==============================] - 4s 117us/step - loss: 2.0364 - acc: 0.3036\n",
      "Epoch 10/20\n",
      "36000/36000 [==============================] - 4s 116us/step - loss: 2.0180 - acc: 0.3125\n",
      "Epoch 11/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 2.0034 - acc: 0.3225\n",
      "Epoch 12/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9925 - acc: 0.3237\n",
      "Epoch 13/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9816 - acc: 0.3311\n",
      "Epoch 14/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9662 - acc: 0.3358\n",
      "Epoch 15/20\n",
      "36000/36000 [==============================] - 4s 118us/step - loss: 1.9529 - acc: 0.3453\n",
      "Epoch 16/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9442 - acc: 0.3444\n",
      "Epoch 17/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9283 - acc: 0.3526\n",
      "Epoch 18/20\n",
      "36000/36000 [==============================] - 4s 120us/step - loss: 1.9313 - acc: 0.3522\n",
      "Epoch 19/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9068 - acc: 0.3636\n",
      "Epoch 20/20\n",
      "36000/36000 [==============================] - 4s 119us/step - loss: 1.9036 - acc: 0.3643\n",
      "4000/4000 [==============================] - 1s 128us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.027604729652405, 0.33625]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 167, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 163, 128)     64128       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 162, 128)     76928       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 161, 128)     89728       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 32, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 27, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 23, 128)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 128)      0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 27, 128)      0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 23, 128)      0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 28, 128)      82048       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 22, 128)      98432       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 17, 128)      114816      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 5, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 3, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 2, 128)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5, 128)       0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 128)      0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 10, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1280)         0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          163968      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 13)           1677        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 691,725\n",
      "Trainable params: 691,725\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (5, 6, 7):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(window)(conv_1x)\n",
    "        dropout_1x = Dropout(0.3)(max_pool_1x)\n",
    "        conv_2x = Conv1D(128, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(window)(conv_2x)\n",
    "        dropout_2x = Dropout(0.3)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    if len(layers) > 1:\n",
    "        merged = Concatenate(axis=1)(layers)\n",
    "    else:\n",
    "        merged = layers[0]\n",
    "\n",
    "    dropout = Dropout(0.3)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36000/36000 [==============================] - 8s 233us/step - loss: 2.1989 - acc: 0.2295\n",
      "Epoch 2/20\n",
      "36000/36000 [==============================] - 7s 199us/step - loss: 2.1366 - acc: 0.2474\n",
      "Epoch 3/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 2.1195 - acc: 0.2499\n",
      "Epoch 4/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 2.0977 - acc: 0.2612\n",
      "Epoch 5/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 2.0674 - acc: 0.2761\n",
      "Epoch 6/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 2.0460 - acc: 0.2846\n",
      "Epoch 7/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 2.0127 - acc: 0.3014\n",
      "Epoch 8/20\n",
      "36000/36000 [==============================] - 7s 198us/step - loss: 1.9956 - acc: 0.3112\n",
      "Epoch 9/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.9721 - acc: 0.3209\n",
      "Epoch 10/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.9470 - acc: 0.3289 2s - loss: 1.9563 \n",
      "Epoch 11/20\n",
      "36000/36000 [==============================] - 7s 196us/step - loss: 1.9302 - acc: 0.3378\n",
      "Epoch 12/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.9157 - acc: 0.3419\n",
      "Epoch 13/20\n",
      "36000/36000 [==============================] - 7s 196us/step - loss: 1.8888 - acc: 0.3538\n",
      "Epoch 14/20\n",
      "36000/36000 [==============================] - 7s 196us/step - loss: 1.8700 - acc: 0.3597\n",
      "Epoch 15/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.8519 - acc: 0.3680\n",
      "Epoch 16/20\n",
      "36000/36000 [==============================] - 7s 194us/step - loss: 1.8311 - acc: 0.3750\n",
      "Epoch 17/20\n",
      "36000/36000 [==============================] - 7s 195us/step - loss: 1.8199 - acc: 0.3799\n",
      "Epoch 18/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.7943 - acc: 0.3877\n",
      "Epoch 19/20\n",
      "36000/36000 [==============================] - 7s 197us/step - loss: 1.7722 - acc: 0.3966\n",
      "Epoch 20/20\n",
      "36000/36000 [==============================] - 7s 196us/step - loss: 1.7599 - acc: 0.4022 2s - loss: 1.767\n",
      "4000/4000 [==============================] - 1s 203us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0358053636550903, 0.329]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and preparing our data\n",
    "\n",
    "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(emotion_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while to load\n",
    "\n",
    "w2v, idf = nb_utils.load_w2v(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(emotion_df['content'])\n",
    "tokens = pad_sequences(tokens)\n",
    "\n",
    "\n",
    "training_count = int(0.9 * len(char_vectors))\n",
    "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
    "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ohtamans/.pyenv/versions/3.6.7/envs/cookbook/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1364: calling reduce_any (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 300)    0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 1)      50000       message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 300)          0           masking_1[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          38528       combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 13)           1677        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,090,205\n",
      "Trainable params: 40,205\n",
      "Non-trainable params: 15,050,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
    "    if weights is not None:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=weights.shape[1], \n",
    "                                weights=[weights], trainable=False, \n",
    "                                name='%s/embedding' % name)\n",
    "    else:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=embedding_size,\n",
    "                                name='%s/embedding' % name)\n",
    "\n",
    "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
    "    assert not (embedding_size is None and embedding_weights is None)\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='message')\n",
    "    \n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)\n",
    "    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        embedding, idf = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
    "    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=categories,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
    "                                     embedding_weights=w2v,\n",
    "                                     idf_weights=idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36000/36000 [==============================] - 3s 93us/step - loss: 2.3522 - acc: 0.2849\n",
      "Epoch 2/10\n",
      "36000/36000 [==============================] - 3s 83us/step - loss: 1.9863 - acc: 0.3357\n",
      "Epoch 3/10\n",
      "36000/36000 [==============================] - 3s 82us/step - loss: 1.9451 - acc: 0.3432\n",
      "Epoch 4/10\n",
      "36000/36000 [==============================] - 3s 82us/step - loss: 1.9184 - acc: 0.3540\n",
      "Epoch 5/10\n",
      "36000/36000 [==============================] - 3s 83us/step - loss: 1.8962 - acc: 0.3606\n",
      "Epoch 6/10\n",
      "36000/36000 [==============================] - 3s 82us/step - loss: 1.8755 - acc: 0.3645\n",
      "Epoch 7/10\n",
      "36000/36000 [==============================] - 3s 81us/step - loss: 1.8573 - acc: 0.3708\n",
      "Epoch 8/10\n",
      "36000/36000 [==============================] - 3s 82us/step - loss: 1.8404 - acc: 0.3794\n",
      "Epoch 9/10\n",
      "36000/36000 [==============================] - 3s 81us/step - loss: 1.8232 - acc: 0.3817\n",
      "Epoch 10/10\n",
      "36000/36000 [==============================] - 3s 82us/step - loss: 1.8114 - acc: 0.3880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19482867f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4151585874557493, 0.302]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Embeddings\n",
    "\n",
    "It looks like our model with pre-trained embeddings isn't doing much better than the linear models.\n",
    "\n",
    "We can also try training a model \"from scratch\", and learn the word embeddings from our training data.  Note that we use a small embedding size here to speed up training and to try to avoid overfitting.\n",
    "\n",
    "Only training for 10 epochs stops the model while it is still improving on the training set, but prevents it\n",
    "from overfitting.  We can formalize this by using a validation set and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 25)     0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 25)           0           masking_2[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          3328        combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 13)           1677        dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,505,005\n",
      "Trainable params: 2,505,005\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36000/36000 [==============================] - 2s 54us/step - loss: 2.1477 - acc: 0.2474\n",
      "Epoch 2/10\n",
      "36000/36000 [==============================] - 1s 40us/step - loss: 1.9916 - acc: 0.3148\n",
      "Epoch 3/10\n",
      "36000/36000 [==============================] - 1s 40us/step - loss: 1.8847 - acc: 0.3586\n",
      "Epoch 4/10\n",
      "36000/36000 [==============================] - 1s 41us/step - loss: 1.8153 - acc: 0.3848\n",
      "Epoch 5/10\n",
      "36000/36000 [==============================] - 1s 41us/step - loss: 1.7606 - acc: 0.4050\n",
      "Epoch 6/10\n",
      "36000/36000 [==============================] - 1s 40us/step - loss: 1.7069 - acc: 0.4228\n",
      "Epoch 7/10\n",
      "36000/36000 [==============================] - 1s 40us/step - loss: 1.6527 - acc: 0.4425\n",
      "Epoch 8/10\n",
      "36000/36000 [==============================] - 1s 41us/step - loss: 1.5968 - acc: 0.4628\n",
      "Epoch 9/10\n",
      "36000/36000 [==============================] - 1s 40us/step - loss: 1.5410 - acc: 0.4846\n",
      "Epoch 10/10\n",
      "36000/36000 [==============================] - 1s 41us/step - loss: 1.4833 - acc: 0.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19156797f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0061434240341187, 0.35275]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the test set accuracy is lower than that on the training set.\n",
    "\n",
    "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Models\n",
    "\n",
    "As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,\n",
    "                              mask_zero=False)\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(128, 3)\n",
    "    cnn_2 = layers.Convolution1D(128, 3)\n",
    "    cnn_3 = layers.Convolution1D(128, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
    "\n",
    "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
    "    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[categories],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    title[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 128)    115328      message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, None, 128)    0           conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    49280       max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 128)    49280       max_pooling1d_11[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 13)           1677        dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,232,077\n",
      "Trainable params: 232,077\n",
      "Non-trainable params: 15,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36000/36000 [==============================] - 5s 151us/step - loss: 1.9774 - acc: 0.3191\n",
      "Epoch 2/10\n",
      "36000/36000 [==============================] - 5s 135us/step - loss: 1.8585 - acc: 0.3614\n",
      "Epoch 3/10\n",
      "36000/36000 [==============================] - 5s 135us/step - loss: 1.7879 - acc: 0.3825\n",
      "Epoch 4/10\n",
      "36000/36000 [==============================] - 5s 136us/step - loss: 1.7079 - acc: 0.4063\n",
      "Epoch 5/10\n",
      "36000/36000 [==============================] - 5s 135us/step - loss: 1.6052 - acc: 0.4404\n",
      "Epoch 6/10\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.4896 - acc: 0.4804\n",
      "Epoch 7/10\n",
      "36000/36000 [==============================] - 5s 133us/step - loss: 1.3629 - acc: 0.5242\n",
      "Epoch 8/10\n",
      "36000/36000 [==============================] - 5s 136us/step - loss: 1.2432 - acc: 0.5617\n",
      "Epoch 9/10\n",
      "36000/36000 [==============================] - 5s 136us/step - loss: 1.1338 - acc: 0.6051\n",
      "Epoch 10/10\n",
      "36000/36000 [==============================] - 5s 135us/step - loss: 1.0401 - acc: 0.6369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19157b81d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 81us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.817425817489624, 0.323]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
    "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
    "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[category],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 15,221,325\n",
      "Trainable params: 221,325\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36000/36000 [==============================] - 17s 473us/step - loss: 2.0171 - acc: 0.3113\n",
      "Epoch 2/10\n",
      "36000/36000 [==============================] - 16s 433us/step - loss: 1.8921 - acc: 0.3528\n",
      "Epoch 3/10\n",
      "36000/36000 [==============================] - 15s 428us/step - loss: 1.8470 - acc: 0.3660\n",
      "Epoch 4/10\n",
      "36000/36000 [==============================] - 15s 430us/step - loss: 1.8153 - acc: 0.3769\n",
      "Epoch 5/10\n",
      "36000/36000 [==============================] - 16s 437us/step - loss: 1.7911 - acc: 0.3839\n",
      "Epoch 6/10\n",
      "36000/36000 [==============================] - 16s 442us/step - loss: 1.7656 - acc: 0.3933\n",
      "Epoch 7/10\n",
      "36000/36000 [==============================] - 15s 418us/step - loss: 1.7425 - acc: 0.4011\n",
      "Epoch 8/10\n",
      "36000/36000 [==============================] - 15s 420us/step - loss: 1.7172 - acc: 0.4080\n",
      "Epoch 9/10\n",
      "36000/36000 [==============================] - 15s 421us/step - loss: 1.6907 - acc: 0.4175\n",
      "Epoch 10/10\n",
      "36000/36000 [==============================] - 15s 422us/step - loss: 1.6620 - acc: 0.4261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1913112080>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 2s 550us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8966071195602416, 0.38475]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models\n",
    "\n",
    "Let's compare the predictions from our models on a sample of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'lstm': lstm_model.predict(test_tokens[:100]),\n",
    "    'char_cnn': char_cnn_model.predict(test_char_vectors[:100]),\n",
    "    'cnn': cnn_model.predict(test_tokens[:100]),\n",
    "    'unigram': unigram_model.predict(test_tokens[:100]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>true</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>char_cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
       "      <td>love</td>\n",
       "      <td>relief</td>\n",
       "      <td>relief</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Mother's Day to all the Mommiessss</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mattgarner haha what's up Matt ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What's up!!? @guillermop</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@nak1a &amp;quot;If there's a camel up a hill&amp;quot; and &amp;quot;I'll give you plankton&amp;quot; ....HILARIOUS!!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@Bern_morley LOL I love your kids</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           content  \\\n",
       "0                                      HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.   \n",
       "1                                                                  browsing thru adopting agencies, i'm gonna get some exotic kids   \n",
       "2  I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...   \n",
       "3                                                                                         Happy Mother's Day to all the Mommiessss   \n",
       "4                                                                                                @mattgarner haha what's up Matt ?   \n",
       "5                                                                                                         What's up!!? @guillermop   \n",
       "6                                       @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.   \n",
       "7  @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...   \n",
       "8                           @nak1a &quot;If there's a camel up a hill&quot; and &quot;I'll give you plankton&quot; ....HILARIOUS!!   \n",
       "9                                                                                                @Bern_morley LOL I love your kids   \n",
       "\n",
       "         true       lstm      cnn   char_cnn    unigram  \n",
       "0   happiness       love     love       love       love  \n",
       "1  enthusiasm    neutral    worry      worry      worry  \n",
       "2        love     relief   relief  happiness       love  \n",
       "3        love       love     love       love  happiness  \n",
       "4   happiness    neutral      fun    neutral      worry  \n",
       "5     neutral    neutral  neutral    neutral    neutral  \n",
       "6         fun  happiness      fun  happiness    neutral  \n",
       "7   happiness    neutral    worry      worry      worry  \n",
       "8   happiness  happiness  neutral  happiness    neutral  \n",
       "9        love       love     love       love       love  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe just for test data\n",
    "\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = emotion_df[training_count:training_count+100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['content'],\n",
    "    'true': test_df['sentiment'],\n",
    "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
    "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
    "    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],    \n",
    "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],\n",
    "})\n",
    "eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'char_cnn', 'unigram']]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>true</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>char_cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
       "      <td>love</td>\n",
       "      <td>relief</td>\n",
       "      <td>relief</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mattgarner haha what's up Matt ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@davecandoit dude that honest to god happens to me all the time.. minus the trail mix.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>happiness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Happy Mother's Day to the tweetin' mamas  Nite tweeple!</td>\n",
       "      <td>worry</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.</td>\n",
       "      <td>relief</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                            content  \\\n",
       "0                                       HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.   \n",
       "1                                                                   browsing thru adopting agencies, i'm gonna get some exotic kids   \n",
       "2   I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...   \n",
       "4                                                                                                 @mattgarner haha what's up Matt ?   \n",
       "6                                        @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.   \n",
       "7   @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...   \n",
       "10                                           @davecandoit dude that honest to god happens to me all the time.. minus the trail mix.   \n",
       "12                                                                          Happy Mother's Day to the tweetin' mamas  Nite tweeple!   \n",
       "13                                                       On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties   \n",
       "14                   @xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.   \n",
       "\n",
       "          true       lstm        cnn   char_cnn    unigram  \n",
       "0    happiness       love       love       love       love  \n",
       "1   enthusiasm    neutral      worry      worry      worry  \n",
       "2         love     relief     relief  happiness       love  \n",
       "4    happiness    neutral        fun    neutral      worry  \n",
       "6          fun  happiness        fun  happiness    neutral  \n",
       "7    happiness    neutral      worry      worry      worry  \n",
       "10     sadness    neutral  happiness    sadness    neutral  \n",
       "12       worry       love       love       love  happiness  \n",
       "13   happiness    neutral  happiness  happiness  happiness  \n",
       "14      relief    neutral    neutral    sadness    neutral  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     40000\n",
       "sentiment    40000\n",
       "author       40000\n",
       "content      40000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets\n",
    "\n",
    "We can gather a sample of Twitter data using the Twitter API (https://dev.twitter.com).  To do so, we'll need to create a Twitter application and get credentials for it.  You can do this manually at https://app.twitter.com.  Once you have an app, go to the \"Key and Access Tokens\" tab to find your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Twitter(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))\n",
    "\n",
    "stream = twitter.TwitterStream(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 23 ms, total: 222 ms\n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time st = list(itertools.islice(filter(has_emoji, stream.statuses.sample()), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['@JTMusicTeam Congrats my fav peeps. You guys just keep surprising  each and everyday!! LOVE YOU!!',\n",
       "  '@grace_ashe Omg Stop get out of here!! Youre too nice',\n",
       "  'The best advice!!!! Thank you doctor love ',\n",
       "  'So one piece will be on break next week.',\n",
       "  'RT @Sporf:  @ManUtd managers win %:\\n\\n\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f Sir Alex Ferguson\\n 59%\\n\\n Jose Mourinho \\n 58%\\n\\n\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f Ernest Mangnall\\n 54%\\n\\n\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f Davi',\n",
       "  \"RT @mihyochaeng: I'm still thinking about this  #MiChaeng https://t.co/Dj9cp60GvQ\",\n",
       "  'RT @DayswithDae: Jongdae                                 me\\n                           \\n               \\n                  nothing i just',\n",
       "  'RT @Tee_Jaruji: 27 December 2018 , SBFIVE  SPARK (...) \\n#SparkSBFIVE #SBFIVE #Starhunterstudio https://t.co/PLvWxjI6d6',\n",
       "  'RT @Mo3tadilaCBA: For all CBA students your coffee during finals is on us!\\nAt Java Coffee and Muse, coffeesphere in Kuwait City \\nJust show',\n",
       "  '@AbdullaAlsuliti Its so obvious wtf '])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st), [t.get('text', None) for t in st][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Emojis\n",
    "\n",
    "fetch many emoji-tweets and save as 'data/emojis.txt' for next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(itertools.islice(\n",
    "    filter(has_emoji, stream.statuses.sample()), 0, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = []\n",
    "for tweet in tweets:\n",
    "    text = tweet['text']\n",
    "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
    "    if len(emojis) == 1:\n",
    "        emoiji = emojis.pop()\n",
    "        text = ''.join(ch for ch in text if ch != emoiji)\n",
    "        stripped.append((text, emoiji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(stripped).to_csv('data/emojis.txt', header=['text', 'emoji'], index=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
